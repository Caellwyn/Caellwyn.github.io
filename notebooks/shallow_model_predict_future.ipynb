{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the right model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from src.functions import load_OU_data, CourseScaler, plot_confusion, score_grid, \\\n",
    "smotecourses, process_courses, course_cross_validate, Course_GridSearchCV\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_style('white')\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_presentation(model, df, show_scores = False, show_progress = False):\n",
    "    \n",
    "    index = df.groupby(by=['code_module','code_presentation']).count().index\n",
    "    scores = pd.DataFrame(columns = ['accuracy'], index = index)\n",
    "    for module in df['code_module'].unique():\n",
    "        df_course = df[df['code_module'] == module]\n",
    "        for presentation in df_course['code_presentation'].unique():\n",
    "            train = df[(df['code_module'] != module) | (df['code_presentation'] != presentation)]\n",
    "            test = df[(df['code_module'] == module) & (df['code_presentation'] == presentation)]\n",
    "            if module in train['code_module'].unique():\n",
    "                score = model_evaluate_presentation(model, train, test)\n",
    "                scores.loc[(module,presentation), 'accuracy'] = score\n",
    "                if show_progress:\n",
    "                    print((module,presentation))\n",
    "            else:\n",
    "                scores.loc[(module,presentation), 'accuracy'] = np.nan\n",
    "    if show_scores:\n",
    "        display(scores.accuracy.mean())\n",
    "    return scores\n",
    "    \n",
    "def model_evaluate_presentation(model, train, test):\n",
    "\n",
    "    X_train = train.drop(columns = ['id_student','code_presentation','region','highest_education', \\\n",
    "                       'imd_band','gender','age_band','disability','studied_credits',\n",
    "                       'module_presentation_length','date_registration','final_result'])\n",
    "    y_train = train['final_result']\n",
    "    X_test = test.drop(columns = ['id_student','code_presentation','region','highest_education', \\\n",
    "                       'imd_band','gender','age_band','disability','studied_credits',\n",
    "                       'module_presentation_length','date_registration','final_result'])\n",
    "    y_test = test['final_result']\n",
    "\n",
    "    transformed_big_data = process_courses(X_train, y_train, X_test, y_test)\n",
    "    X_train_transformed, y_train_transformed, X_test_transformed, y_test = transformed_big_data\n",
    "    model.fit(X_train_transformed, y_train_transformed)\n",
    "    y_pred = model.predict(X_test_transformed)\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = load_OU_data(prediction_window=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_full.copy()\n",
    "df['final_result'] = np.array(['No Intervention' if w in ['Pass','Distinction'] \\\n",
    "              else 'Needs Intervention' for w in df['final_result']])\n",
    "test_df = df[df['code_presentation'] == '2014J']\n",
    "train_df = df[df['code_presentation'] != '2014J']\n",
    "X_test = test_df.drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo:\n",
    "1. data_prep() function\n",
    "2. X_test, y_test (holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id_student</th>\n",
       "      <th>gender</th>\n",
       "      <th>region</th>\n",
       "      <th>highest_education</th>\n",
       "      <th>imd_band</th>\n",
       "      <th>age_band</th>\n",
       "      <th>num_of_prev_attempts</th>\n",
       "      <th>studied_credits</th>\n",
       "      <th>disability</th>\n",
       "      <th>final_result</th>\n",
       "      <th>date_registration</th>\n",
       "      <th>module_presentation_length</th>\n",
       "      <th>days_studied</th>\n",
       "      <th>activities_engaged</th>\n",
       "      <th>total_clicks</th>\n",
       "      <th>assessments_completed</th>\n",
       "      <th>average_assessment_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code_module</th>\n",
       "      <th>code_presentation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAA</th>\n",
       "      <th>2013J</th>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">BBB</th>\n",
       "      <th>2013B</th>\n",
       "      <td>1363</td>\n",
       "      <td>1363</td>\n",
       "      <td>1363</td>\n",
       "      <td>1363</td>\n",
       "      <td>1363</td>\n",
       "      <td>1363</td>\n",
       "      <td>1363</td>\n",
       "      <td>1363</td>\n",
       "      <td>1363</td>\n",
       "      <td>1363</td>\n",
       "      <td>1363</td>\n",
       "      <td>1363</td>\n",
       "      <td>1363</td>\n",
       "      <td>1363</td>\n",
       "      <td>1363</td>\n",
       "      <td>1363</td>\n",
       "      <td>1363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013J</th>\n",
       "      <td>1681</td>\n",
       "      <td>1681</td>\n",
       "      <td>1681</td>\n",
       "      <td>1681</td>\n",
       "      <td>1681</td>\n",
       "      <td>1681</td>\n",
       "      <td>1681</td>\n",
       "      <td>1681</td>\n",
       "      <td>1681</td>\n",
       "      <td>1681</td>\n",
       "      <td>1681</td>\n",
       "      <td>1681</td>\n",
       "      <td>1681</td>\n",
       "      <td>1681</td>\n",
       "      <td>1681</td>\n",
       "      <td>1681</td>\n",
       "      <td>1681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014B</th>\n",
       "      <td>1181</td>\n",
       "      <td>1181</td>\n",
       "      <td>1181</td>\n",
       "      <td>1181</td>\n",
       "      <td>1181</td>\n",
       "      <td>1181</td>\n",
       "      <td>1181</td>\n",
       "      <td>1181</td>\n",
       "      <td>1181</td>\n",
       "      <td>1181</td>\n",
       "      <td>1181</td>\n",
       "      <td>1181</td>\n",
       "      <td>1181</td>\n",
       "      <td>1181</td>\n",
       "      <td>1181</td>\n",
       "      <td>1181</td>\n",
       "      <td>1181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCC</th>\n",
       "      <th>2014B</th>\n",
       "      <td>1137</td>\n",
       "      <td>1137</td>\n",
       "      <td>1137</td>\n",
       "      <td>1137</td>\n",
       "      <td>1137</td>\n",
       "      <td>1137</td>\n",
       "      <td>1137</td>\n",
       "      <td>1137</td>\n",
       "      <td>1137</td>\n",
       "      <td>1137</td>\n",
       "      <td>1137</td>\n",
       "      <td>1137</td>\n",
       "      <td>1137</td>\n",
       "      <td>1137</td>\n",
       "      <td>1137</td>\n",
       "      <td>1137</td>\n",
       "      <td>1137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DDD</th>\n",
       "      <th>2013B</th>\n",
       "      <td>947</td>\n",
       "      <td>947</td>\n",
       "      <td>947</td>\n",
       "      <td>947</td>\n",
       "      <td>947</td>\n",
       "      <td>947</td>\n",
       "      <td>947</td>\n",
       "      <td>947</td>\n",
       "      <td>947</td>\n",
       "      <td>947</td>\n",
       "      <td>947</td>\n",
       "      <td>947</td>\n",
       "      <td>947</td>\n",
       "      <td>947</td>\n",
       "      <td>947</td>\n",
       "      <td>947</td>\n",
       "      <td>947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013J</th>\n",
       "      <td>1392</td>\n",
       "      <td>1392</td>\n",
       "      <td>1392</td>\n",
       "      <td>1392</td>\n",
       "      <td>1392</td>\n",
       "      <td>1392</td>\n",
       "      <td>1392</td>\n",
       "      <td>1392</td>\n",
       "      <td>1392</td>\n",
       "      <td>1392</td>\n",
       "      <td>1392</td>\n",
       "      <td>1392</td>\n",
       "      <td>1392</td>\n",
       "      <td>1392</td>\n",
       "      <td>1392</td>\n",
       "      <td>1392</td>\n",
       "      <td>1392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014B</th>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "      <td>846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">EEE</th>\n",
       "      <th>2013J</th>\n",
       "      <td>818</td>\n",
       "      <td>818</td>\n",
       "      <td>818</td>\n",
       "      <td>818</td>\n",
       "      <td>818</td>\n",
       "      <td>818</td>\n",
       "      <td>818</td>\n",
       "      <td>818</td>\n",
       "      <td>818</td>\n",
       "      <td>818</td>\n",
       "      <td>818</td>\n",
       "      <td>818</td>\n",
       "      <td>818</td>\n",
       "      <td>818</td>\n",
       "      <td>818</td>\n",
       "      <td>818</td>\n",
       "      <td>818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014B</th>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FFF</th>\n",
       "      <th>2013B</th>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013J</th>\n",
       "      <td>1671</td>\n",
       "      <td>1671</td>\n",
       "      <td>1671</td>\n",
       "      <td>1671</td>\n",
       "      <td>1671</td>\n",
       "      <td>1671</td>\n",
       "      <td>1671</td>\n",
       "      <td>1671</td>\n",
       "      <td>1671</td>\n",
       "      <td>1671</td>\n",
       "      <td>1671</td>\n",
       "      <td>1671</td>\n",
       "      <td>1671</td>\n",
       "      <td>1671</td>\n",
       "      <td>1671</td>\n",
       "      <td>1671</td>\n",
       "      <td>1671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014B</th>\n",
       "      <td>1106</td>\n",
       "      <td>1106</td>\n",
       "      <td>1106</td>\n",
       "      <td>1106</td>\n",
       "      <td>1106</td>\n",
       "      <td>1106</td>\n",
       "      <td>1106</td>\n",
       "      <td>1106</td>\n",
       "      <td>1106</td>\n",
       "      <td>1106</td>\n",
       "      <td>1106</td>\n",
       "      <td>1106</td>\n",
       "      <td>1106</td>\n",
       "      <td>1106</td>\n",
       "      <td>1106</td>\n",
       "      <td>1106</td>\n",
       "      <td>1106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">GGG</th>\n",
       "      <th>2013J</th>\n",
       "      <td>907</td>\n",
       "      <td>907</td>\n",
       "      <td>907</td>\n",
       "      <td>907</td>\n",
       "      <td>907</td>\n",
       "      <td>907</td>\n",
       "      <td>907</td>\n",
       "      <td>907</td>\n",
       "      <td>907</td>\n",
       "      <td>907</td>\n",
       "      <td>907</td>\n",
       "      <td>907</td>\n",
       "      <td>907</td>\n",
       "      <td>907</td>\n",
       "      <td>907</td>\n",
       "      <td>907</td>\n",
       "      <td>907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014B</th>\n",
       "      <td>774</td>\n",
       "      <td>774</td>\n",
       "      <td>774</td>\n",
       "      <td>774</td>\n",
       "      <td>774</td>\n",
       "      <td>774</td>\n",
       "      <td>774</td>\n",
       "      <td>774</td>\n",
       "      <td>774</td>\n",
       "      <td>774</td>\n",
       "      <td>774</td>\n",
       "      <td>774</td>\n",
       "      <td>774</td>\n",
       "      <td>774</td>\n",
       "      <td>774</td>\n",
       "      <td>774</td>\n",
       "      <td>774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id_student  gender  region  highest_education  \\\n",
       "code_module code_presentation                                                  \n",
       "AAA         2013J                     338     338     338                338   \n",
       "BBB         2013B                    1363    1363    1363               1363   \n",
       "            2013J                    1681    1681    1681               1681   \n",
       "            2014B                    1181    1181    1181               1181   \n",
       "CCC         2014B                    1137    1137    1137               1137   \n",
       "DDD         2013B                     947     947     947                947   \n",
       "            2013J                    1392    1392    1392               1392   \n",
       "            2014B                     846     846     846                846   \n",
       "EEE         2013J                     818     818     818                818   \n",
       "            2014B                     543     543     543                543   \n",
       "FFF         2013B                    1231    1231    1231               1231   \n",
       "            2013J                    1671    1671    1671               1671   \n",
       "            2014B                    1106    1106    1106               1106   \n",
       "GGG         2013J                     907     907     907                907   \n",
       "            2014B                     774     774     774                774   \n",
       "\n",
       "                               imd_band  age_band  num_of_prev_attempts  \\\n",
       "code_module code_presentation                                             \n",
       "AAA         2013J                   338       338                   338   \n",
       "BBB         2013B                  1363      1363                  1363   \n",
       "            2013J                  1681      1681                  1681   \n",
       "            2014B                  1181      1181                  1181   \n",
       "CCC         2014B                  1137      1137                  1137   \n",
       "DDD         2013B                   947       947                   947   \n",
       "            2013J                  1392      1392                  1392   \n",
       "            2014B                   846       846                   846   \n",
       "EEE         2013J                   818       818                   818   \n",
       "            2014B                   543       543                   543   \n",
       "FFF         2013B                  1231      1231                  1231   \n",
       "            2013J                  1671      1671                  1671   \n",
       "            2014B                  1106      1106                  1106   \n",
       "GGG         2013J                   907       907                   907   \n",
       "            2014B                   774       774                   774   \n",
       "\n",
       "                               studied_credits  disability  final_result  \\\n",
       "code_module code_presentation                                              \n",
       "AAA         2013J                          338         338           338   \n",
       "BBB         2013B                         1363        1363          1363   \n",
       "            2013J                         1681        1681          1681   \n",
       "            2014B                         1181        1181          1181   \n",
       "CCC         2014B                         1137        1137          1137   \n",
       "DDD         2013B                          947         947           947   \n",
       "            2013J                         1392        1392          1392   \n",
       "            2014B                          846         846           846   \n",
       "EEE         2013J                          818         818           818   \n",
       "            2014B                          543         543           543   \n",
       "FFF         2013B                         1231        1231          1231   \n",
       "            2013J                         1671        1671          1671   \n",
       "            2014B                         1106        1106          1106   \n",
       "GGG         2013J                          907         907           907   \n",
       "            2014B                          774         774           774   \n",
       "\n",
       "                               date_registration  module_presentation_length  \\\n",
       "code_module code_presentation                                                  \n",
       "AAA         2013J                            338                         338   \n",
       "BBB         2013B                           1363                        1363   \n",
       "            2013J                           1681                        1681   \n",
       "            2014B                           1181                        1181   \n",
       "CCC         2014B                           1137                        1137   \n",
       "DDD         2013B                            947                         947   \n",
       "            2013J                           1392                        1392   \n",
       "            2014B                            846                         846   \n",
       "EEE         2013J                            818                         818   \n",
       "            2014B                            543                         543   \n",
       "FFF         2013B                           1231                        1231   \n",
       "            2013J                           1671                        1671   \n",
       "            2014B                           1106                        1106   \n",
       "GGG         2013J                            907                         907   \n",
       "            2014B                            774                         774   \n",
       "\n",
       "                               days_studied  activities_engaged  total_clicks  \\\n",
       "code_module code_presentation                                                   \n",
       "AAA         2013J                       338                 338           338   \n",
       "BBB         2013B                      1363                1363          1363   \n",
       "            2013J                      1681                1681          1681   \n",
       "            2014B                      1181                1181          1181   \n",
       "CCC         2014B                      1137                1137          1137   \n",
       "DDD         2013B                       947                 947           947   \n",
       "            2013J                      1392                1392          1392   \n",
       "            2014B                       846                 846           846   \n",
       "EEE         2013J                       818                 818           818   \n",
       "            2014B                       543                 543           543   \n",
       "FFF         2013B                      1231                1231          1231   \n",
       "            2013J                      1671                1671          1671   \n",
       "            2014B                      1106                1106          1106   \n",
       "GGG         2013J                       907                 907           907   \n",
       "            2014B                       774                 774           774   \n",
       "\n",
       "                               assessments_completed  average_assessment_score  \n",
       "code_module code_presentation                                                   \n",
       "AAA         2013J                                338                       338  \n",
       "BBB         2013B                               1363                      1363  \n",
       "            2013J                               1681                      1681  \n",
       "            2014B                               1181                      1181  \n",
       "CCC         2014B                               1137                      1137  \n",
       "DDD         2013B                                947                       947  \n",
       "            2013J                               1392                      1392  \n",
       "            2014B                                846                       846  \n",
       "EEE         2013J                                818                       818  \n",
       "            2014B                                543                       543  \n",
       "FFF         2013B                               1231                      1231  \n",
       "            2013J                               1671                      1671  \n",
       "            2014B                               1106                      1106  \n",
       "GGG         2013J                                907                       907  \n",
       "            2014B                                774                       774  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(by=['code_module','code_presentation']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('BBB', '2013B')\n",
      "('BBB', '2013J')\n",
      "('BBB', '2014B')\n",
      "('DDD', '2013B')\n",
      "('DDD', '2013J')\n",
      "('DDD', '2014B')\n",
      "('EEE', '2013J')\n",
      "('EEE', '2014B')\n",
      "('FFF', '2013B')\n",
      "('FFF', '2013J')\n",
      "('FFF', '2014B')\n",
      "('GGG', '2013J')\n",
      "('GGG', '2014B')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x27caeb94ac0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = LogisticRegression()\n",
    "scores = cross_val_presentation(model, train_df, show_scores = False, show_progress=True)\n",
    "scores.plot(kind = 'bar', figsize = (10,5), title = 'Logistic Regression Accuracy by Presentation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.759112184606397"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBClassifier()\n",
    "cross_val_presentation(model, df, show_scores = False, show_progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AAA', '2013J')\n",
      "('AAA', '2014J')\n",
      "('BBB', '2013B')\n",
      "('BBB', '2013J')\n",
      "('BBB', '2014B')\n",
      "('BBB', '2014J')\n",
      "('CCC', '2014B')\n",
      "('CCC', '2014J')\n",
      "('DDD', '2013B')\n",
      "('DDD', '2013J')\n",
      "('DDD', '2014B')\n",
      "('DDD', '2014J')\n",
      "('EEE', '2013J')\n",
      "('EEE', '2014B')\n",
      "('EEE', '2014J')\n",
      "('FFF', '2013B')\n",
      "('FFF', '2013J')\n",
      "('FFF', '2014B')\n",
      "('FFF', '2014J')\n",
      "('GGG', '2013J')\n",
      "('GGG', '2014B')\n",
      "('GGG', '2014J')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7613645869842576"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pickle.load(open('../models/XGBmodel4.pkl','rb'))\n",
    "cross_val_presentation(model, df, show_scores = False, show_progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4450 entries, 3706 to 23206\n",
      "Data columns (total 6 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   num_of_prev_attempts      4450 non-null   float64\n",
      " 1   days_studied              4450 non-null   float64\n",
      " 2   activities_engaged        4450 non-null   float64\n",
      " 3   total_clicks              4450 non-null   float64\n",
      " 4   assessments_completed     4450 non-null   float64\n",
      " 5   average_assessment_score  4450 non-null   float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 243.4 KB\n"
     ]
    }
   ],
   "source": [
    "X_val_transformed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4450"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CourseScaler class and smotecourses() function\n",
    "We run into a problem with our data because this dataset comprises 7 different courses and each course has different numbers of activities, assessments, and demands different amounts of work.  This creates a variable in the data which is unrelated to the actual student activity or effort, but is intrinsic in the course itself.  We can partially over come this, making our model more general, by normalizing the activity statistics for each course.  In other words, while 100 activities may be sufficient to succeed in course A, 200 activities may be necessary for course B.  Instead, we take the mean of all students' activitiy counts and divide by the standard deviation.  Now the necessary activities for course A and B are set to the same value, more or less.  \n",
    "\n",
    "This is what the `CourseScaler` transformer does. It can fit on and transform the training data and then use the mean and standard deviation from the training data to transform the test data on the same scale.\n",
    "\n",
    "But, different courses don't also have different work requirements, they also have different success rates, which will also bias the predictions.  The solution is to balance the classes in each course to remove this bias as well, which is what the function `smotecourses()` does.  SMOTE is a class from [Imbalanced Learn](https://imbalanced-learn.org/stable/index.html) which creates synthetic data as a way of upsampling a minority class without directly duplicating observations.  It uses a K-neartest neighbors approach to create new observations with very similar features to others.  Once the minority class is smoted, the classes are balanced in each course and the bias is removed.  For the purposes of training our model, each course has the same graduation rate and requires the same level of effort and grades assessments on the same scale.  See the readme for a more on why this is important.\n",
    "\n",
    "I wrap these into the `process_courses()` function, which returns scaled and smoted training set and a test set scaled on the CourseScaler fitted on the train set (but not smoted)\n",
    "\n",
    "With these solutions we can overcome the bias that different courses would have otherwise introduced into our dataset.\n",
    "\n",
    "# When do we transform?\n",
    "We are going to create transformed versions of the above datasets for use in evaluating models.  However our custom course_cross_validate function will automatically perform these transformations on each cross-validation fold during it's hyperparameter search to prevent data leakage during cross validation.  When we cross-validate models we will only give them the X_t and y_t datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSM\n",
    "A decent start for a baseline model, a logistic regresson model seeks the best fit line to model the linear relationship between the predictor variables, X, and the target variable, y, which is a linear regression. It then applies a sigmoid function to that line to assign probabilities that each observation belongs in one class or the other.  For our purposes, if the probability of an observation belonging a class is greater than .5, then we will predict that it belongs to that class.  We will use our custom cross_validate function to remove the course bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7724637681159421, 0.7830917874396135, 0.7864734299516908, 0.7805703238279362, 0.7849202513291446]\n",
      "Mean cross validated accuracy:\n",
      "0.7815039121328654\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'AAA'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-a4befceb3551>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Mean cross validated accuracy:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_transformed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'validation accuracy: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\new_Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1340\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1342\u001b[1;33m         X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0m\u001b[0;32m   1343\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1344\u001b[0m                                    accept_large_sparse=solver != 'liblinear')\n",
      "\u001b[1;32m~\\new_Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\new_Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\new_Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    794\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    797\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\new_Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\new_Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    597\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\new_Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\new_Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1780\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1781\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1783\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\new_Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'AAA'"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "scores = course_cross_validate(lr,X_t,y_t,scoring='accuracy', cv=cv, random_state = 111)\n",
    "\n",
    "print(scores)\n",
    "print('Mean cross validated accuracy:')\n",
    "print(np.mean(scores))\n",
    "lr.fit(X_t, y_t)\n",
    "y_pred = lr.predict(X_val_transformed)\n",
    "print('validation accuracy: ')\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "plot_confusion(y_val, y_pred, cmap='Greens', save_path='../figures/FSMconfusionmatrix.png')\n",
    "pickle.dump(lr,open('../models/FSM2.pkl','wb'))\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "The FSM achieves 73.6% accuracy with a good balance between course predictions.  This is a lovely start, but let's see if we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridsearchCV for best logistic regression hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRparams = [{'solver': ['lbfgs','sag','saga','newton-cg'],\n",
    "         'penalty': ['none'],\n",
    "         'random_state':[111]},\n",
    "          {'solver': ['lbfgs','sag','saga','newton-cg','liblinear'],\n",
    "         'penalty': ['l2'],\n",
    "         'random_state':[111]},\n",
    "          {'solver':['saga'],\n",
    "          'penalty':['elasticnet'], 'l1_ratio':[.1,.5,.7]},\n",
    "          {'solver':['saga','liblinear'],\n",
    "          'penalty':['l1']}]\n",
    "\n",
    "\n",
    "LRgrid = Course_GridSearchCV(LogisticRegression(), LRparams, cv=cv, \n",
    "                                scoring='accuracy', verbose = False)\n",
    "\n",
    "LRgrid.fit(X_t, y_t)\n",
    "LRmodel = score_grid(LRgrid, X_val_transformed, y_val)\n",
    "pickle.dump(LRmodel,open('../models/LRmodel2.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Evaluation\n",
    "By optimizing the hyperparameters of the logistic regression model, which are all regularization parameters, we've barely moved the needle at all.  We gained .0002 accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More model types\n",
    "We used the logistic regression classifier to tune our features, but now it's time to try some other models.  We will use Course_GridsearchCV to optimize the hyperparameters for these as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "A decision tree is a promising candidate for this problem because it does not assume the independence of the features. Instead it seeks to find the best way to divide and subdivide the data in a tree structure based on the values of different variables.  We can tune how many features each split is allowed to consider.  Once the tree is build predictions are made by sending an observations down the tree sending it on a path to the predicted class as it reaches each split in the tree is sent in one or the other direction.  You can think of it like a deterministic Pachinko machine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTparams = {'criterion':['gini','entropy'],\n",
    "           'splitter':['best','random'],\n",
    "           'max_depth':[4,10],'min_samples_split':[.01,.1,.2,.5],\n",
    "           'max_features':[1,2,3,4,5],'random_state':[111]}\n",
    "           \n",
    "DTgrid = Course_GridSearchCV(DecisionTreeClassifier(), DTparams, cv=cv, scoring = 'accuracy')\n",
    "\n",
    "DTgrid.fit(X_t, y_t)\n",
    "\n",
    "DTmodel = score_grid(DTgrid, X_val_transformed, y_val, cmap = 'Greens')\n",
    "plt.savefig('../figures/DTconfmatrix.png',dpi=250)\n",
    "pickle.dump(DTmodel,open('../models/DTmodel2.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Evaluation \n",
    "As expected, this model is better at predicting how a student will do.  It doesn't rely on establishing a straight line through the data to model it, which is probably not the best approach in this problem space.  We see a nice increase in accuracy here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier\n",
    "This is an interesting extension to the decision tree model.  It creates a whole forest of decision trees and trains each one on a subset of the data and a subset of the features.  This is a technique called bagging, or [Boostrap AGGregation](https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/) (check the link for more on this).  It works on the principle that a bunch of bad predictors, on average will be more accurate than one good predictor.  This worked for Francis Galton in [guessing the weight of an ox](https://crowdsourcingweek.com/blog/using-the-crowd-to-predict/), maybe it will work here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFparams = {'n_estimators':[10,100,150],\n",
    "           'criterion':['gini','entropy'],\n",
    "           'max_depth':[4],\n",
    "           'min_samples_split':[2,4,5],\n",
    "           'oob_score':[True,False],\n",
    "           'n_jobs':[-1],'random_state':[111],\n",
    "           'max_samples':[.5,.7,None]}\n",
    "           \n",
    "RFgrid = Course_GridSearchCV(RandomForestClassifier(), RFparams, cv=cv)\n",
    "\n",
    "RFgrid.fit(X_t, y_t)\n",
    "\n",
    "RFmodel = score_grid(RFgrid, X_val_transformed, y_val, save_path = '../figures/RFmodel2confmatrix.png')\n",
    "pickle.dump(RFmodel, open('../models/RFmodel2.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier Evaluation:\n",
    "Apparently bagging is not the right approach to this problem.  Our single well tuned decision tree outperformed the random forest classifier.  I guess the wisdom of the crowd is not always superior to the wisdom of the expert!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eXtreme Gradient Boost model:\n",
    "XGBoost models have gained a lot of popularity recently and won a lot of Kaggle competitions.  It uses another popular idea called [boosting](https://en.wikipedia.org/wiki/Gradient_boosting).  That's a pretty involved wikipedia article, but the TLDR is that it's a similar ensemble method like random forest, but whereas random forest trains a bunch of trees in parallel and takes the aggregate of their predictions, boosting stacks the trees on top of each other and each one tries to improve on the one below it by predicting where the previous one made mistakes.  I think of it as like a line of morons each grading the next one's paper, which is an analysis of the previous one's paper.  Each one gets a lot wrong, but something right so the right answers percolate through and some of the wrong answers get corrected at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBparams = {'n_estimators':[150,200],\n",
    "             'max_depth':[4],\n",
    "             'learning_rate':[.1], \n",
    "             'subsample':[.7],\n",
    "             'gamma':[0,1],\n",
    "             'min_child_weight':[1,2],\n",
    "             'num_parallel_tree':[1,2],\n",
    "             'eval_metric':['error','logloss'],\n",
    "            'colsample_bytree':[.6,.7,.8],\n",
    "            'base_score':[.1,.2,.4]}\n",
    "           \n",
    "XBGgrid = Course_GridSearchCV(XGBClassifier(objective='binary:logistic', n_jobs=-1, random_state=111),\n",
    "                         XGBparams, cv=cv)\n",
    "XBGgrid.fit(X_t, y_t)\n",
    "\n",
    "XGBmodel = score_grid(XBGgrid, X_val_transformed, y_val, \n",
    "                     save_path = '../figures/XGBmodelconfmatrix4.png')\n",
    "pickle.dump(XGBmodel, open('../models/XGBmodel4.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost evaluation:\n",
    "XGBoost gave us the best cross-validated score of these four model types.  While it performs slightly worse on the validation set than the random forest, looking at the scores across folds, it seems likely that this represents the validation set being a sort of bad split.  XGBoost also performs equally well on both classes.  It correctly identifies 74% of students who need intervention to succeed, while misclassifying 27% of passing students as needing intervention.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "An SVM, or support vector machine, attempts to draw a line, or hyperplane, between the classes to divide them.  This can be degree 2 polynomial line or higher, creating curved dividers of different shapes to match more complex decision boundaries.  It finds the best fit such hyperplane so as to maximize the distance between observations in a recursive manner.  EDA suggests that the observations can, to a large extent, be divided along the variables we have seen.  Decision trees, which do not operate this way, have been relatively successful, but let's see what this model, somewhat more closely related to the logistic regression, can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVCparams = {'C':[.01],\n",
    "            'kernel':['linear',],\n",
    "             'gamma':['auto'],\n",
    "             'tol':[1e-3],}\n",
    "             \n",
    "SVCgrid = Course_GridSearchCV(SVC(random_state=111), SVCparams, cv=cv, \n",
    "                                scoring='accuracy', verbose = True)\n",
    "\n",
    "SVCgrid.fit(X_t, y_t)\n",
    "SVCmodel = score_grid(SVCgrid, X_val_transformed, y_val)\n",
    "pickle.dump(SVCmodel, open('../models/SVCmodel1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Evaluation\n",
    "The SVM performed quite well.  It scored an overall accuracy of 77% with slight prediction bias toward 'No Intervention', in line with the other models.  \n",
    "\n",
    "Different hyper parameters didn't make much difference to accuracy, except a polynomial kernel (degree 3) over predicted student success.  This would be the way to go if interventions are particularly costly.  If you biased the predictions that much toward 'No Intervention', you could be fairly certain that the students receiving interventions really need them.  The same results could also be achieved with the logistic regression model by setting the decision point of the sigmoid function toward the passing class, biasing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "This model positions each observation (student registration in this case) from the training set in n_feature dimensional space.  New observations from the test set are voted on by the closest K observations from the training set to determine which class the new observation should belong to. There are two big benefits to this modeling technique for our dataset.  The first is that it is fast with few features.  We have only 6 features, so the dimensionality of the space is low, observations are more tightly packed, and the model makes determinations quickly.  The second benefit is that it is non-linear.  We've had some success with more linear models, and decision tree models.  Our error analysis and EDA show that there are strong linear relationships between our variables and the success of students.  However there is a solid 20% of students that cannot seem to be classified with these linear models.  The decision tree models similar seem to fail to classify these students.  Perhaps this model can help find where they belong.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNparams = {'n_neighbors':[30, 50, 100, 150],\n",
    "            'weights':['distance'],\n",
    "             'p':[6, 7, 8, 9],\n",
    "             'n_jobs':[-1],}\n",
    "             \n",
    "KNNgrid = Course_GridSearchCV(KNeighborsClassifier(), KNNparams, cv=cv, \n",
    "                                scoring='accuracy', verbose = False)\n",
    "\n",
    "KNNgrid.fit(X_t, y_t)\n",
    "KNNmodel = score_grid(KNNgrid, X_val_transformed, y_val)\n",
    "pickle.dump(KNNmodel, open('../models/KNNmodel1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Evaluation\n",
    "This model, like the others, seems to top out around 77%.  We still haven't found a way to capture the pattern for how those last 20-23% of students fail or withdraw.  Looking at our error analysis from the final report notebook we see that those misclassfied students really look a lot like successful ones according to all of our variables.  It may just be that we don't have the right variable to find the connection.  We may be running up against the irreducible error for this feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model:\n",
    "Our XGboost model performed the best here, so lets give it the whole training set to learn from and see how it does on our hold-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = pickle.load(open('../models/XGBmodel4.pkl','rb'))\n",
    "\n",
    "final_model.fit(X_train_transformed, y_train_transformed)\n",
    "y_pred = final_model.predict(X_test_transformed)\n",
    "print('Final Model')\n",
    "print(final_model)\n",
    "print('Final Model Classification Report')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Final Model Confusion Matrix')\n",
    "plot_confusion(y_test, y_pred, save_path='../figures/final_modelconfmatrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Evaluation:\n",
    "Our final model, the XGBoost, does pretty well on the test set.  We could have hoped for more, but this is pretty good.  It identifies almost 80% of students in danger of who will do fine while misclassifying a little more than a quarter of students needing interventions.  This may not be ready for deployment, but I believe it serves as a solid proof of concept that student success can be, in many cases, predicted by their interactions with a virtual learning environment.  The difference in different models was, for the most part, quite small.  The XGBoost only won out by 2% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFmodel.fit(X_train_transformed,y_train_transformed)\n",
    "y_pred = RFmodel.predict(X_test_transformed)\n",
    "plot_confusion(y_test, y_pred)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTmodel.fit(X_train_transformed,y_train_transformed)\n",
    "y_pred = DTmodel.predict(X_test_transformed)\n",
    "plot_confusion(y_test, y_pred)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRmodel.fit(X_train_transformed,y_train_transformed)\n",
    "y_pred = LRmodel.predict(X_test_transformed)\n",
    "plot_confusion(y_test, y_pred)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVCmodel.fit(X_train_transformed,y_train_transformed)\n",
    "y_pred = SVCmodel.predict(X_test_transformed)\n",
    "plot_confusion(y_test, y_pred)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBmodel.fit(X_train_transformed,y_train_transformed)\n",
    "y_pred = XGBmodel.predict(X_test_transformed)\n",
    "plot_confusion(y_test, y_pred)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNmodel.fit(X_train_transformed,y_train_transformed)\n",
    "y_pred = KNNmodel.predict(X_test_transformed)\n",
    "plot_confusion(y_test, y_pred)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
